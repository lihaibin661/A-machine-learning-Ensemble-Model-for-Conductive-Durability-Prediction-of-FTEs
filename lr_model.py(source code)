# lr_model.py
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error

# Load data for the LR model from "更新线性回归600.csv"
data = pd.read_csv('data/更新线性回归600.csv')  # Adjust the path accordingly

# For LR, we assume the last three columns are input features and column index 6 is the target variable.
X = data.iloc[:, -3:]  # Use the last three columns as features
y = data.iloc[:, 6]    # Use column index 6 as the target

# Impute missing values using the mean
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Expand features with a second-degree polynomial (without bias)
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_scaled)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

# Build a simple neural network as the LR model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(1)
])
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the LR model for 100 epochs (with 20% validation split)
history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)

# Predict on the test set
predictions = model.predict(X_test).flatten()

# Evaluate the model
mse = mean_squared_error(y_test, predictions)
print("LR Model Test MSE: {:.4f}".format(mse))

# Save the predictions for the stacking ensemble
pd.DataFrame({'lr_pred': predictions}).to_csv('lr_predictions.csv', index=False)
